# UHOK ML Inference Service

ë ˆì‹œí”¼ ì¶”ì²œì„ ìœ„í•œ ì„ë² ë”© ìƒì„± ML ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ğŸ¯ ëª©ì 

- **ë¹„ìš© ìµœì í™”**: ë¬´ê±°ìš´ ML ëª¨ë¸ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ EC2 ë¹„ìš© ì ˆì•½
- **í™•ì¥ì„±**: ML ì„œë¹„ìŠ¤ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
- **ìœ ì§€ë³´ìˆ˜ì„±**: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”

## ğŸ”§ ì½”ë“œ êµ¬ì¡° ë° ê¸°ëŠ¥

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

#### 1. **app/main.py** - FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 
- **ê¸°ëŠ¥**: FastAPI ì„œë²„ ì„¤ì • ë° CORS ë¯¸ë“¤ì›¨ì–´ êµ¬ì„±
- **ì£¼ìš” ì—­í• **:
  - ì„œë¹„ìŠ¤ ë©”íƒ€ë°ì´í„° ì •ì˜ (ì œëª©, ì„¤ëª…, ë²„ì „)
  - CORS ì„¤ì •ìœ¼ë¡œ í¬ë¡œìŠ¤ ì˜¤ë¦¬ì§„ ìš”ì²­ í—ˆìš©
  - API ë¼ìš°í„° ë“±ë¡ (`/api/v1` í”„ë¦¬í”½ìŠ¤)
  - í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ (`/health`) ì œê³µ
  - uvicorn ì„œë²„ ì‹¤í–‰ (í¬íŠ¸ 8001)

#### 2. **app/api.py** - REST API ì—”ë“œí¬ì¸íŠ¸
- **ê¸°ëŠ¥**: ì„ë² ë”© ìƒì„± ë° ë°°ì¹˜ ì²˜ë¦¬ API ì œê³µ
- **ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸**:
  - `POST /api/v1/embed`: ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
  - `POST /api/v1/embed-batch`: ë‹¤ì¤‘ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„±
  - `GET /api/v1/model-info`: í˜„ì¬ ëª¨ë¸ ì •ë³´ ì¡°íšŒ
- **ë°ì´í„° ëª¨ë¸**:
  - `EmbedRequest`: ë‹¨ì¼ ì„ë² ë”© ìš”ì²­ (í…ìŠ¤íŠ¸, ì •ê·œí™” ì—¬ë¶€)
  - `EmbedBatchRequest`: ë°°ì¹˜ ì„ë² ë”© ìš”ì²­ (í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸, ìµœëŒ€ 100ê°œ)
  - `EmbedResponse`: ì„ë² ë”© ì‘ë‹µ (ë²¡í„°, ì°¨ì›, ë²„ì „)
  - `EmbedBatchResponse`: ë°°ì¹˜ ì„ë² ë”© ì‘ë‹µ (ë²¡í„° ë¦¬ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ê° ìš”ì²­ì˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…

#### 3. **app/deps.py** - ML ëª¨ë¸ ì˜ì¡´ì„± ê´€ë¦¬
- **ê¸°ëŠ¥**: SentenceTransformer ëª¨ë¸ì˜ ë¡œë”©, ìºì‹±, ì„ë² ë”© ìƒì„±
- **ì£¼ìš” í•¨ìˆ˜**:
  - `get_model()`: ì „ì—­ ëª¨ë¸ ìºì‹œ ê´€ë¦¬ (ì‹±ê¸€í†¤ íŒ¨í„´)
  - `encode_text()`: í…ìŠ¤íŠ¸ë¥¼ 384ì°¨ì› ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
  - `get_model_info()`: ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë°˜í™˜
- **ëª¨ë¸ ì •ë³´**:
  - ëª¨ë¸ëª…: `paraphrase-multilingual-MiniLM-L12-v2`
  - ì°¨ì›: 384ì°¨ì›
  - ë””ë°”ì´ìŠ¤: CPU ì „ìš©
  - ë²„ì „: sentence-transformers-5.0.0
- **ë™ì‹œì„± ì œì–´**: asyncio.Lockì„ ì‚¬ìš©í•œ ìŠ¤ë ˆë“œ ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©

#### 4. **test_ml_service.py** - í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- **ê¸°ëŠ¥**: ML ì„œë¹„ìŠ¤ì˜ ëª¨ë“  API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
- **í…ŒìŠ¤íŠ¸ í•­ëª©**:
  - í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸
  - ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© í…ŒìŠ¤íŠ¸ (5ê°œ ìƒ˜í”Œ í…ìŠ¤íŠ¸)
  - ë°°ì¹˜ ì„ë² ë”© í…ŒìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ í…ìŠ¤íŠ¸)
  - ëª¨ë¸ ì •ë³´ ì¡°íšŒ í…ŒìŠ¤íŠ¸
- **ì„±ëŠ¥ ì¸¡ì •**: ê° ìš”ì²­ì˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë° í†µê³„ ì œê³µ
- **ì—ëŸ¬ ì²˜ë¦¬**: íƒ€ì„ì•„ì›ƒ ë° HTTP ì—ëŸ¬ ì²˜ë¦¬

### ê¸°ìˆ  ìŠ¤íƒ

#### **ì›¹ í”„ë ˆì„ì›Œí¬**
- **FastAPI**: ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ì›¹ í”„ë ˆì„ì›Œí¬
- **Uvicorn**: ASGI ì„œë²„ (í‘œì¤€ WSGI ëŒ€ì‹  ë¹„ë™ê¸° ì§€ì›)
- **Pydantic**: ë°ì´í„° ê²€ì¦ ë° ì§ë ¬í™”

#### **ML/AI ë¼ì´ë¸ŒëŸ¬ë¦¬**
- **SentenceTransformers**: ë¬¸ì¥ ì„ë² ë”© ìƒì„± (í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- **PyTorch**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (CPU ì „ìš©)
- **HuggingFace Hub**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê´€ë¦¬
- **Transformers**: HuggingFace íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### **ë°ì´í„° ì²˜ë¦¬**
- **NumPy**: ìˆ˜ì¹˜ ê³„ì‚° ë° ë°°ì—´ ì²˜ë¦¬
- **httpx**: ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ (í…ŒìŠ¤íŠ¸ìš©)

### í•µì‹¬ ê¸°ëŠ¥

#### **1. ì„ë² ë”© ìƒì„±**
- í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ 384ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- ì •ê·œí™” ì˜µì…˜ ì§€ì› (L2 ì •ê·œí™”)
- ë‹¨ì¼ ë° ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë‘ ì§€ì›

#### **2. ëª¨ë¸ ê´€ë¦¬**
- ì „ì—­ ìºì‹œë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©
- ì²« ìš”ì²­ ì‹œì—ë§Œ ëª¨ë¸ ë¡œë”© (ì½œë“œìŠ¤íƒ€íŠ¸ ìµœì†Œí™”)
- ë™ì‹œì„± ì•ˆì „í•œ ëª¨ë¸ ì ‘ê·¼

#### **3. API ì„¤ê³„**
- RESTful API ì„¤ê³„ ì›ì¹™ ì¤€ìˆ˜
- ëª…í™•í•œ ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜
- ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ë° ë¡œê¹…

#### **4. ì„±ëŠ¥ ìµœì í™”**
- CPU ì „ìš© PyTorch ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬
- ëª¨ë¸ ìºì‹±ìœ¼ë¡œ ë°˜ë³µ ë¡œë”© ë°©ì§€

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
[Backend Service] --HTTP--> [ML Inference Service]
     â†“                              â†“
[PostgreSQL]                 [SentenceTransformer]
[pgvector]                   [paraphrase-multilingual-MiniLM-L12-v2]
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Dockerë¡œ ì‹¤í–‰
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t uhok-ml-inference .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8001:8001 uhok-ml-inference
```

### Docker Composeë¡œ ì‹¤í–‰

#### ë¡œì»¬ ê°œë°œ í™˜ê²½
```bash
cd uhok-ml-inference
docker-compose -f docker-compose.ml.yml up -d
```

#### í†µí•© í™˜ê²½ (uhok-deployì™€ í•¨ê»˜)
```bash
cd uhok-deploy
docker-compose --profile with-ml up -d
```

## ğŸ“¡ API ì‚¬ìš©ë²•

### í—¬ìŠ¤ ì²´í¬
```bash
curl http://localhost:8001/health
```

### ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
```bash
curl -X POST http://localhost:8001/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ê°ˆë¹„íƒ•", "normalize": true}'
```

### ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
```bash
curl -X POST http://localhost:8001/api/v1/embed-batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"], "normalize": true}'
```

## ğŸ”§ ê°œë°œ í™˜ê²½ ì„¤ì •

### ë¡œì»¬ ê°œë°œ

#### Python ì§ì ‘ ì‹¤í–‰
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œ ì„œë²„ ì‹¤í–‰
python -m app.main
```

#### Docker Compose ì‚¬ìš© (ê¶Œì¥)
```bash
# ML ì„œë¹„ìŠ¤ë§Œ ë…ë¦½ ì‹¤í–‰
cd uhok-ml-inference
docker-compose -f docker-compose.ml.yml up --build

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
docker-compose -f docker-compose.ml.yml up -d

# ë¡œê·¸ í™•ì¸
docker-compose -f docker-compose.ml.yml logs -f

# ì„œë¹„ìŠ¤ ì¤‘ì§€
docker-compose -f docker-compose.ml.yml down
```

### í™˜ê²½ ë³€ìˆ˜
```bash
# HuggingFace ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
export HF_HOME=/models/hf_cache

# Python ê²½ë¡œ ì„¤ì •
export PYTHONPATH=/app
```

## ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

- **ëª¨ë¸**: paraphrase-multilingual-MiniLM-L12-v2 (384ì°¨ì›)
- **ì²˜ë¦¬ëŸ‰**: CPU ê¸°ë°˜, ë‹¨ì¼ ì›Œì»¤
- **ì§€ì—°ì‹œê°„**: ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë”© ì‹œê°„ í¬í•¨
- **ë©”ëª¨ë¦¬**: ì•½ 1-2GB (ëª¨ë¸ + ëŸ°íƒ€ì„)

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ê¸°ë³¸ í…ŒìŠ¤íŠ¸
```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8001/health

# ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ê°ˆë¹„íƒ•", "normalize": true}'

# ë°°ì¹˜ ì„ë² ë”© í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/v1/embed-batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"], "normalize": true}'

# ëª¨ë¸ ì •ë³´ ì¡°íšŒ
curl http://localhost:8001/api/v1/model-info
```

### í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```bash
# uhok-deploy ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
python test_ml_integration.py
```

## ğŸ”„ ë°±ì—”ë“œ ì—°ë™

ë°±ì—”ë“œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì›ê²© ML ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤:

```python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ML_INFERENCE_URL=http://ml-inference:8001
ML_TIMEOUT=30.0  # ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
ML_RETRIES=2

# ì›ê²© ì„ë² ë”© í˜¸ì¶œ
async with httpx.AsyncClient(timeout=ML_TIMEOUT) as client:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/embed",
        json={"text": query, "normalize": True}
    )
    response.raise_for_status()
    result = response.json()
    embedding = result["embedding"]
    dim = result["dim"]
```

### ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/embed",
        json={"text": query, "normalize": True}
    )
    response.raise_for_status()
    return response.json()["embedding"]
except httpx.TimeoutException:
    logger.error("ML ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ")
    return None
except httpx.HTTPStatusError as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ HTTP ì—ëŸ¬: {e.response.status_code}")
    return None
except Exception as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    return None
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸
```bash
# Docker Composeë¡œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° (ë…ë¦½ ì‹¤í–‰)
docker-compose -f docker-compose.ml.yml logs -f

# í†µí•© í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
docker-compose logs -f ml-inference

# ì§ì ‘ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
python -m app.main
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats uhok-ml-inference

# CPU ì‚¬ìš©ëŸ‰ í™•ì¸
docker exec uhok-ml-inference top
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ì²« ìš”ì²­ ì§€ì—°**: ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•œ ì½œë“œìŠ¤íƒ€íŠ¸ (ì•½ 10-30ì´ˆ)
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•œ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš© (1-2GB)
3. **ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„±**: ë°±ì—”ë“œì™€ ML ì„œë¹„ìŠ¤ ê°„ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í•„ìš”
4. **ì—ëŸ¬ ì²˜ë¦¬**: ML ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ í•„ìš”
5. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ê³ ë ¤í•œ ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì • í•„ìš”
6. **ë™ì‹œì„±**: ë‹¨ì¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¸í•œ ì²˜ë¦¬ëŸ‰ ì œí•œ

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ìºì‹œ ê´€ë¦¬
```bash
# HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
ls -la ~/.cache/huggingface/

# ìºì‹œ ì •ë¦¬ (í•„ìš”ì‹œ)
rm -rf ~/.cache/huggingface/
```

### ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
```bash
# ë…ë¦½ ì‹¤í–‰ í™˜ê²½ì—ì„œ ì—°ê²° í…ŒìŠ¤íŠ¸
curl http://localhost:8001/health

# í†µí•© í™˜ê²½ì—ì„œ ë°±ì—”ë“œì—ì„œ ML ì„œë¹„ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
docker-compose exec backend ping ml-inference

# í¬íŠ¸ í™•ì¸
docker-compose exec backend telnet ml-inference 8001
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²°
```bash
# ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
docker run -m 4g -p 8001:8001 uhok-ml-inference

# ë˜ëŠ” docker-compose.ml.ymlì—ì„œ
services:
  ml-inference:
    deploy:
      resources:
        limits:
          memory: 4G
```

## ğŸ”„ ë²„ì „ ê´€ë¦¬

### ë²„ì „ ì—…ê·¸ë ˆì´ë“œ
```bash
# 1. docker-compose.ml.ymlì—ì„œ ì´ë¯¸ì§€ ë²„ì „ ìˆ˜ì •
# image: uhok-ml-inference:1.0.1 â†’ uhok-ml-inference:1.0.2

# 2. ìƒˆ ì´ë¯¸ì§€ ë¹Œë“œ
docker-compose -f docker-compose.ml.yml build --no-cache

# 3. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose -f docker-compose.ml.yml down
docker-compose -f docker-compose.ml.yml up -d
```

### ë¡¤ë°±
```bash
# ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
# docker-compose.ml.ymlì—ì„œ ì´ì „ ë²„ì „ìœ¼ë¡œ ìˆ˜ì • í›„
docker-compose -f docker-compose.ml.yml down
docker-compose -f docker-compose.ml.yml up -d
```

## ğŸ“š API ë¬¸ì„œ

ìì„¸í•œ API ë¬¸ì„œëŠ” ì„œë¹„ìŠ¤ ì‹¤í–‰ í›„ ë‹¤ìŒ URLì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´:
1. ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”
2. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: `docker-compose logs -f ml-inference`
3. í—¬ìŠ¤ì²´í¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: `curl http://localhost:8001/health`