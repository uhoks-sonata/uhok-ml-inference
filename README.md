# UHOK ML Inference Service

ë ˆì‹œí”¼ ì¶”ì²œì„ ìœ„í•œ ì„ë² ë”© ìƒì„± ML ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ğŸ¯ ëª©ì 

- **ë¹„ìš© ìµœì í™”**: ë¬´ê±°ìš´ ML ëª¨ë¸ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ EC2 ë¹„ìš© ì ˆì•½
- **í™•ì¥ì„±**: ML ì„œë¹„ìŠ¤ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
- **ìœ ì§€ë³´ìˆ˜ì„±**: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
[Backend Service] --HTTP--> [ML Inference Service]
     â†“                              â†“
[PostgreSQL]                 [SentenceTransformer]
[pgvector]                   [paraphrase-multilingual-MiniLM-L12-v2]
```

## ğŸš€ API ì—”ë“œí¬ì¸íŠ¸

### í—¬ìŠ¤ ì²´í¬
```http
GET /health
```

### ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
```http
POST /api/v1/embed
Content-Type: application/json

{
  "text": "ê°ˆë¹„íƒ•",
  "normalize": true
}
```

### ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
```http
POST /api/v1/embed-batch
Content-Type: application/json

{
  "texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"],
  "normalize": true
}
```

### ëª¨ë¸ ì •ë³´ ì¡°íšŒ
```http
GET /api/v1/model-info
```

## ğŸ³ Docker ì‹¤í–‰

### ë¡œì»¬ ê°œë°œ
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t uhok-ml-inference .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8001:8001 uhok-ml-inference
```

### Docker Compose
```yaml
services:
  ml-inference:
    build: ./uhok-ml-inference
    ports:
      - "8001:8001"
    environment:
      - HF_HOME=/models/hf_cache
    volumes:
      - ml_cache:/models/hf_cache
```

## ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

- **ëª¨ë¸**: paraphrase-multilingual-MiniLM-L12-v2 (384ì°¨ì›)
- **ì²˜ë¦¬ëŸ‰**: CPU ê¸°ë°˜, ë‹¨ì¼ ì›Œì»¤
- **ì§€ì—°ì‹œê°„**: ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë”© ì‹œê°„ í¬í•¨
- **ë©”ëª¨ë¦¬**: ì•½ 1-2GB (ëª¨ë¸ + ëŸ°íƒ€ì„)

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜
- `HF_HOME`: HuggingFace ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
- `PYTHONPATH`: Python ê²½ë¡œ ì„¤ì •

### ë¡œê¹…
- êµ¬ì¡°í™”ëœ JSON ë¡œê·¸ ì¶œë ¥
- ìš”ì²­/ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- ì—ëŸ¬ ìƒì„¸ ì •ë³´ í¬í•¨

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8001/health

# ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ê°ˆë¹„íƒ•", "normalize": true}'
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

- **í—¬ìŠ¤ì²´í¬**: `/health` ì—”ë“œí¬ì¸íŠ¸
- **ë©”íŠ¸ë¦­**: ìš”ì²­ ìˆ˜, ì‘ë‹µ ì‹œê°„, ì—ëŸ¬ìœ¨
- **ë¡œê·¸**: êµ¬ì¡°í™”ëœ JSON í˜•íƒœ

## ğŸ”„ ë°±ì—”ë“œ ì—°ë™

ë°±ì—”ë“œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì›ê²© ML ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤:

```python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ML_INFERENCE_URL=http://ml-inference:8001
ML_TIMEOUT=3.0
ML_RETRIES=2

# ì›ê²© ì„ë² ë”© í˜¸ì¶œ
async with httpx.AsyncClient(timeout=ML_TIMEOUT) as client:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/embed",
        json={"text": query, "normalize": True}
    )
    embedding = response.json()["embedding"]
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ì²« ìš”ì²­ ì§€ì—°**: ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•œ ì½œë“œìŠ¤íƒ€íŠ¸ (ì•½ 10-30ì´ˆ)
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•œ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
3. **ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„±**: ë°±ì—”ë“œì™€ ML ì„œë¹„ìŠ¤ ê°„ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í•„ìš”
4. **ì—ëŸ¬ ì²˜ë¦¬**: ML ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ í•„ìš”
