# UHOK ML Inference Service

ë ˆì‹œí”¼ ì¶”ì²œì„ ìœ„í•œ ì„ë² ë”© ìƒì„± ML ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ğŸ¯ ëª©ì 

- **ë¹„ìš© ìµœì í™”**: ë¬´ê±°ìš´ ML ëª¨ë¸ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ EC2 ë¹„ìš© ì ˆì•½
- **í™•ì¥ì„±**: ML ì„œë¹„ìŠ¤ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
- **ìœ ì§€ë³´ìˆ˜ì„±**: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
[Backend Service] --HTTP--> [ML Inference Service]
     â†“                              â†“
[PostgreSQL]                 [SentenceTransformer]
[pgvector]                   [paraphrase-multilingual-MiniLM-L12-v2]
```

## ğŸš€ API ì—”ë“œí¬ì¸íŠ¸

### í—¬ìŠ¤ ì²´í¬
```http
GET /health
```

**ì‘ë‹µ:**
```json
{
  "status": "ok",
  "model": "paraphrase-multilingual-MiniLM-L12-v2",
  "dim": 384,
  "version": "sentence-transformers-5.0.0"
}
```

### ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
```http
POST /api/v1/embed
Content-Type: application/json

{
  "text": "ê°ˆë¹„íƒ•",
  "normalize": true
}
```

**ì‘ë‹µ:**
```json
{
  "embedding": [0.123, -0.456, 0.789, ...],
  "dim": 384,
  "version": "sentence-transformers-5.0.0"
}
```

### ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
```http
POST /api/v1/embed-batch
Content-Type: application/json

{
  "texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"],
  "normalize": true
}
```

**ì‘ë‹µ:**
```json
{
  "embeddings": [[0.123, -0.456, ...], [0.234, -0.567, ...], [0.345, -0.678, ...]],
  "dim": 384,
  "version": "sentence-transformers-5.0.0",
  "count": 3
}
```

### ëª¨ë¸ ì •ë³´ ì¡°íšŒ
```http
GET /api/v1/model-info
```

**ì‘ë‹µ:**
```json
{
  "model_name": "paraphrase-multilingual-MiniLM-L12-v2",
  "dimension": 384,
  "version": "sentence-transformers-5.0.0",
  "device": "cpu"
}
```

## ğŸ³ Docker ì‹¤í–‰

### ë¡œì»¬ ê°œë°œ
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t uhok-ml-inference .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8001:8001 uhok-ml-inference
```

### Docker Compose
```yaml
services:
  ml-inference:
    build: ./uhok-ml-inference
    ports:
      - "8001:8001"
    environment:
      - HF_HOME=/models/hf_cache
    volumes:
      - ml_cache:/models/hf_cache
```

## ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

- **ëª¨ë¸**: paraphrase-multilingual-MiniLM-L12-v2 (384ì°¨ì›)
- **ì²˜ë¦¬ëŸ‰**: CPU ê¸°ë°˜, ë‹¨ì¼ ì›Œì»¤
- **ì§€ì—°ì‹œê°„**: ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë”© ì‹œê°„ í¬í•¨
- **ë©”ëª¨ë¦¬**: ì•½ 1-2GB (ëª¨ë¸ + ëŸ°íƒ€ì„)

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜
- `HF_HOME`: HuggingFace ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
- `PYTHONPATH`: Python ê²½ë¡œ ì„¤ì •

### ë¡œê¹…
- êµ¬ì¡°í™”ëœ JSON ë¡œê·¸ ì¶œë ¥
- ìš”ì²­/ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- ì—ëŸ¬ ìƒì„¸ ì •ë³´ í¬í•¨

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ê¸°ë³¸ í…ŒìŠ¤íŠ¸
```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8001/health

# ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ê°ˆë¹„íƒ•", "normalize": true}'

# ë°°ì¹˜ ì„ë² ë”© í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/v1/embed-batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"], "normalize": true}'

# ëª¨ë¸ ì •ë³´ ì¡°íšŒ
curl http://localhost:8001/api/v1/model-info
```

### í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```bash
# uhok-deploy ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
python test_ml_integration.py
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

- **í—¬ìŠ¤ì²´í¬**: `/health` ì—”ë“œí¬ì¸íŠ¸
- **ë©”íŠ¸ë¦­**: ìš”ì²­ ìˆ˜, ì‘ë‹µ ì‹œê°„, ì—ëŸ¬ìœ¨
- **ë¡œê·¸**: êµ¬ì¡°í™”ëœ JSON í˜•íƒœ

## ğŸ”„ ë°±ì—”ë“œ ì—°ë™

ë°±ì—”ë“œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì›ê²© ML ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤:

```python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ML_INFERENCE_URL=http://ml-inference:8001
ML_TIMEOUT=30.0  # ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
ML_RETRIES=2

# ì›ê²© ì„ë² ë”© í˜¸ì¶œ
async with httpx.AsyncClient(timeout=ML_TIMEOUT) as client:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/embed",
        json={"text": query, "normalize": True}
    )
    response.raise_for_status()
    result = response.json()
    embedding = result["embedding"]
    dim = result["dim"]
```

### ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    response = await client.post(
        f"{ML_INFERENCE_URL}/api/v1/embed",
        json={"text": query, "normalize": True}
    )
    response.raise_for_status()
    return response.json()["embedding"]
except httpx.TimeoutException:
    logger.error("ML ì„œë¹„ìŠ¤ íƒ€ì„ì•„ì›ƒ")
    return None
except httpx.HTTPStatusError as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ HTTP ì—ëŸ¬: {e.response.status_code}")
    return None
except Exception as e:
    logger.error(f"ML ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    return None
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ì²« ìš”ì²­ ì§€ì—°**: ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•œ ì½œë“œìŠ¤íƒ€íŠ¸ (ì•½ 10-30ì´ˆ)
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•œ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš© (1-2GB)
3. **ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„±**: ë°±ì—”ë“œì™€ ML ì„œë¹„ìŠ¤ ê°„ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í•„ìš”
4. **ì—ëŸ¬ ì²˜ë¦¬**: ML ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ í•„ìš”
5. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ê³ ë ¤í•œ ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì • í•„ìš”
6. **ë™ì‹œì„±**: ë‹¨ì¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¸í•œ ì²˜ë¦¬ëŸ‰ ì œí•œ

## ğŸ”§ ê°œë°œ ë° ë””ë²„ê¹…

### ë¡œê·¸ í™•ì¸
```bash
# Docker Composeë¡œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
docker compose logs -f ml-inference

# ì§ì ‘ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
python -m app.main
```

### ëª¨ë¸ ìºì‹œ ê´€ë¦¬
```bash
# HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
ls -la ~/.cache/huggingface/

# ìºì‹œ ì •ë¦¬ (í•„ìš”ì‹œ)
rm -rf ~/.cache/huggingface/
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats uhok-ml-inference

# CPU ì‚¬ìš©ëŸ‰ í™•ì¸
docker exec uhok-ml-inference top
```
