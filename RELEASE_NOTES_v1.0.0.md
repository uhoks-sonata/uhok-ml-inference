# Uhok ML-Inference v1.0.0 Release Notes

## [v1.0.0] - 2025-09-09

### ğŸ‰ ì²« ë²ˆì§¸ ì •ì‹ ë¦´ë¦¬ìŠ¤

ì´ë²ˆ ë¦´ë¦¬ìŠ¤ëŠ” UHOK ML Inference Serviceì˜ ì²« ë²ˆì§¸ ì •ì‹ ë²„ì „ì…ë‹ˆë‹¤. ë ˆì‹œí”¼ ì¶”ì²œì„ ìœ„í•œ ê³ ì„±ëŠ¥ ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### âœ¨ ìƒˆë¡œìš´ ê¸°ëŠ¥

#### í•µì‹¬ ê¸°ëŠ¥
- **í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±**: í•œêµ­ì–´ ë ˆì‹œí”¼ í…ìŠ¤íŠ¸ë¥¼ 384ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- **ë°°ì¹˜ ì²˜ë¦¬**: ë‹¨ì¼/ë‹¤ì¤‘ í…ìŠ¤íŠ¸ ë™ì‹œ ì²˜ë¦¬ ì§€ì›
- **RESTful API**: FastAPI ê¸°ë°˜ì˜ í‘œì¤€í™”ëœ API ì¸í„°í˜ì´ìŠ¤
- **í—¬ìŠ¤ ì²´í¬**: ì„œë¹„ìŠ¤ ìƒíƒœ ë° ëª¨ë¸ ì •ë³´ ì‹¤ì‹œê°„ í™•ì¸

#### AI/ML ê¸°ëŠ¥
- **ëª¨ë¸**: `paraphrase-multilingual-MiniLM-L12-v2` (SentenceTransformers 5.0.0)
- **ì°¨ì›**: 384ì°¨ì› ë²¡í„° ì„ë² ë”©
- **ì–¸ì–´ ì§€ì›**: í•œêµ­ì–´ ìµœì í™”, ë‹¤êµ­ì–´ ì§€ì›
- **ì •ê·œí™”**: ì„ íƒì  ë²¡í„° ì •ê·œí™” ì§€ì›

#### ì•„í‚¤í…ì²˜
- **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤**: ë°±ì—”ë“œì™€ ë¶„ë¦¬ëœ ë…ë¦½ì  ì„œë¹„ìŠ¤
- **Docker ì§€ì›**: ì»¨í…Œì´ë„ˆí™”ëœ ë°°í¬ ë° ì‹¤í–‰
- **ë¹„ë™ê¸° ì²˜ë¦¬**: FastAPI + Uvicorn ê¸°ë°˜ ê³ ì„±ëŠ¥ ì²˜ë¦¬
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: CPU ì „ìš© PyTorchë¡œ ë¦¬ì†ŒìŠ¤ ìµœì í™”

### ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ

- **FastAPI 0.116.1**: ê³ ì„±ëŠ¥ ì›¹ API í”„ë ˆì„ì›Œí¬
- **PyTorch 2.7.1**: CPU ì „ìš© ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **SentenceTransformers 5.0.0**: ë¬¸ì¥ ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Transformers 4.54.1**: HuggingFace íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸
- **NumPy 2.3.2**: ìˆ˜ì¹˜ ê³„ì‚°

### ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

#### í—¬ìŠ¤ ì²´í¬
```http
GET /health
```

#### ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
```http
POST /api/v1/embed
Content-Type: application/json

{
  "text": "ê°ˆë¹„íƒ•",
  "normalize": true
}
```

#### ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
```http
POST /api/v1/embed-batch
Content-Type: application/json

{
  "texts": ["ê°ˆë¹„íƒ•", "ê¹€ì¹˜ì°Œê°œ", "ëœì¥ì°Œê°œ"],
  "normalize": true
}
```

#### ëª¨ë¸ ì •ë³´ ì¡°íšŒ
```http
GET /api/v1/model-info
```

### ğŸ³ ë°°í¬

#### Docker ì‹¤í–‰
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t uhok-ml-inference .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8001:8001 uhok-ml-inference
```

#### Docker Compose í†µí•©
```yaml
services:
  ml-inference:
    build: ./uhok-ml-inference
    ports:
      - "8001:8001"
    environment:
      - HF_HOME=/models/hf_cache
    volumes:
      - ml_cache:/models/hf_cache
```

### ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

- **ì²« ìš”ì²­**: 10-30ì´ˆ (ëª¨ë¸ ë¡œë”© ì‹œê°„)
- **ì´í›„ ìš”ì²­**: 100-300ms (CPU ê¸°ë°˜)
- **ë©”ëª¨ë¦¬**: 1-2GB (ëª¨ë¸ + ëŸ°íƒ€ì„)
- **ë°°ì¹˜ ì²˜ë¦¬**: 3ê°œ í…ìŠ¤íŠ¸ ë™ì‹œ ì²˜ë¦¬ ì§€ì›

### ğŸ”— ë°±ì—”ë“œ í†µí•©

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
ML_MODE=remote_embed
ML_INFERENCE_URL=http://ml-inference:8001
ML_TIMEOUT=3.0
ML_RETRIES=2
```

#### í†µí•© ë°©ì‹
- HTTP APIë¥¼ í†µí•œ ì„ë² ë”© ìƒì„±
- íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„, í´ë°± ë©”ì»¤ë‹ˆì¦˜
- êµ¬ì¡°í™”ëœ JSON ë¡œê·¸ ì¶œë ¥
- í—¬ìŠ¤ì²´í¬ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

### ğŸ§ª í…ŒìŠ¤íŠ¸

#### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```bash
python test_ml_service.py
```

#### í†µí•© í…ŒìŠ¤íŠ¸
```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8001/health

# ì„ë² ë”© ìƒì„±
curl -X POST http://localhost:8001/api/v1/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ê°ˆë¹„íƒ•", "normalize": true}'
```

### ğŸ“ˆ ëª¨ë‹ˆí„°ë§

- **êµ¬ì¡°í™”ëœ JSON ë¡œê·¸**: íŒŒì‹± ê°€ëŠ¥í•œ ë¡œê·¸ í˜•ì‹
- **ìš”ì²­ ì¶”ì **: ìš”ì²­ ID ê¸°ë°˜ ë¡œê·¸ ì¶”ì 
- **ì„±ëŠ¥ ë©”íŠ¸ë¦­**: ì‘ë‹µ ì‹œê°„, ì²˜ë¦¬ëŸ‰ ì¸¡ì •
- **ì—ëŸ¬ ë¡œê¹…**: ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ë° ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤

### ğŸš¨ ì•Œë ¤ì§„ ì œí•œì‚¬í•­

#### ì„±ëŠ¥ ì œí•œ
- **ì½œë“œìŠ¤íƒ€íŠ¸**: ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•œ ì§€ì—°
- **ë‹¨ì¼ ì›Œì»¤**: ë™ì‹œ ì²˜ë¦¬ ì œí•œ (ìˆœì°¨ ì²˜ë¦¬)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•œ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

#### ìš´ì˜ ì œí•œ
- **ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„±**: ë°±ì—”ë“œì™€ì˜ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í•„ìˆ˜
- **ì—ëŸ¬ ì²˜ë¦¬**: ML ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ ë°±ì—”ë“œ ì¶”ì²œ ê¸°ëŠ¥ ì¤‘ë‹¨
- **ìŠ¤ì¼€ì¼ë§**: ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§ (ìë™ ì˜¤í† ìŠ¤ì¼€ì¼ë§ ë¯¸ì§€ì›)

### ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

#### ê¸°ì¡´ ì‹œìŠ¤í…œì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜
1. ë°±ì—”ë“œ ì„¤ì • ë³€ê²½: `ML_MODE=remote_embed` ì„¤ì •
2. ML ì„œë¹„ìŠ¤ ë°°í¬: Docker Composeë¡œ ML ì„œë¹„ìŠ¤ ì‹¤í–‰
3. ë„¤íŠ¸ì›Œí¬ ì„¤ì •: ë‚´ë¶€ í†µì‹  ì„¤ì • í™•ì¸
4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: í†µí•© í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ê²€ì¦
5. ì ì§„ì  ì „í™˜: íŠ¸ë˜í”½ì„ ì ì§„ì ìœ¼ë¡œ ì „í™˜

#### ë¡¤ë°± ê³„íš
```bash
# ê¸´ê¸‰ ë¡¤ë°± (ë¡œì»¬ ëª¨ë“œë¡œ ë³µê·€)
echo "ML_MODE=local" > uhok-backend/.env
docker-compose restart backend
```

### ğŸ› ï¸ ê°œë°œì ê°€ì´ë“œ

#### ë¡œì»¬ ê°œë°œ í™˜ê²½ ì„¤ì •
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œ ì„œë²„ ì‹¤í–‰
python -m app.main
```

#### ë””ë²„ê¹…
```bash
# ë¡œê·¸ í™•ì¸
docker-compose logs -f ml-inference

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats uhok-ml-inference
```

### ğŸ“ ì§€ì›

#### ë¬¸ì œ í•´ê²°
1. **ë¡œê·¸ í™•ì¸**: `docker-compose logs -f ml-inference`
2. **í—¬ìŠ¤ì²´í¬**: `curl http://localhost:8001/health`
3. **ë„¤íŠ¸ì›Œí¬ í™•ì¸**: `docker-compose exec backend ping ml-inference`
4. **í™˜ê²½ë³€ìˆ˜ í™•ì¸**: `docker-compose exec backend env | grep ML_`

#### ë¬¸ì„œ
- **API ë¬¸ì„œ**: `/docs` ì—”ë“œí¬ì¸íŠ¸ (Swagger UI)
- **í†µí•© ê°€ì´ë“œ**: `INTEGRATION_GUIDE.md`
- **í™˜ê²½ ì„¤ì •**: `ENVIRONMENT_SETUP.md`

### ğŸ¯ í–¥í›„ ê³„íš

#### v1.1.0 (ì˜ˆì •)
- GPU ì§€ì›: CUDA ê¸°ë°˜ ê°€ì† ì²˜ë¦¬
- ë°°ì¹˜ í¬ê¸° ìµœì í™”: ë” í° ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
- ìºì‹±: ì„ë² ë”© ê²°ê³¼ ìºì‹± ê¸°ëŠ¥

#### v1.2.0 (ì˜ˆì •)
- ë‹¤ì¤‘ ëª¨ë¸: ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ì§€ì›
- A/B í…ŒìŠ¤íŠ¸: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê¸°ëŠ¥
- ë©”íŠ¸ë¦­ ìˆ˜ì§‘: Prometheus/Grafana ì—°ë™

### ğŸ† ê¸°ì—¬ì

- **ê°œë°œíŒ€**: @khangte @seosebin 
- **MLíŒ€**: @ziheon-42 

---

**UHOK ML Inference Service v1.0.0** - ë ˆì‹œí”¼ ì¶”ì²œì„ ìœ„í•œ ê³ ì„±ëŠ¥ ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤
